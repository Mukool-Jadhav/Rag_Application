# ðŸ“– RAG Application

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using:

- **ChromaDB** as the vector database for storing and retrieving embeddings.  
- **Gemini Embedding Model** for converting documents into dense vector representations.  
- **Gemini Chat Model** for generating natural language responses grounded on retrieved context.  

The goal is to enable **context-aware Q&A** by combining document retrieval with LLM reasoning.

---

## ðŸš€ Features
- ðŸ”Ž Store and search documents using **ChromaDB**.  
- ðŸ§  Use **Gemini embeddings** to represent documents and queries in vector space.  
- ðŸ’¬ Answer user queries with **Gemini chat model**, enriched by retrieved context.  
- ðŸ“‚ Modular code structure for embeddings, vector storage, and chat pipeline.  
- âš¡ Lightweight and extendable for custom datasets or other vector DBs.  

---

## ðŸ“‚ Project Structure
